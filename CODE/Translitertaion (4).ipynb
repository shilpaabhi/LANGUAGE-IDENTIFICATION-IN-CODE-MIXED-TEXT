{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Translitertaion.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpoW18KxYLY2",
        "outputId": "b10b6f6a-932c-45ab-a31a-7c3a6b2f0ced"
      },
      "source": [
        "!pip install emoji\n",
        "!pip install langdetect"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n",
            "\r\u001b[K     |‚ñà‚ñà‚ñå                             | 10kB 11.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà                           | 20kB 16.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 30kB 11.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 40kB 8.7MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 51kB 5.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 61kB 5.1MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 71kB 5.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 81kB 6.2MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 92kB 6.1MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 102kB 6.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 112kB 6.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 122kB 6.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 133kB 6.6MB/s \n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZGG1akSCFhc"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPH-2UJ8Q_Ub"
      },
      "source": [
        "train = pd.read_csv(\"train.csv\")\n",
        "dev = pd.read_csv(\"dev.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "text = train.text.tolist()\n",
        "text.extend(dev.text.tolist())\n",
        "text.extend(test.text.tolist())\n",
        "\n",
        "with open(\"vocab.txt\",\"w\") as f:\n",
        "  for line in text:\n",
        "    for word in line.split():\n",
        "      f.write(word+\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV1qXEbTCcHP"
      },
      "source": [
        "with open(\"vocab.txt\") as f:\n",
        "  word_set = f.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93SV0ktcDlPy"
      },
      "source": [
        "error_words = set()\n",
        "transliteration_map = {}\n",
        "visited_words = set()\n",
        "\n",
        "for word in word_set:\n",
        "  tword=word.strip(\"\\n\")\n",
        "  if tword not in visited_words:\n",
        "    visited_words.add(tword)\n",
        "    if \"#\" in tword:\n",
        "      tword =tword.replace(\"#\",\"$$$\")\n",
        "    URL = f\"https://www.google.com/inputtools/request?text={tword}&ime=transliteration_en_hi&num=5&cp=0&cs=0&ie=utf-8&oe=utf-8&app=jsapi&uv\"\n",
        "\n",
        "    PARAMS = {} \n",
        "    r = requests.get(url = URL, params = PARAMS)\n",
        "    \n",
        "    data = r.json()\n",
        "    if data[0] != \"SUCCESS\":\n",
        "      error_words.add(tword.replace(\"$$$\",\"#\"))\n",
        "    else:\n",
        "      try:\n",
        "        transliteration_map[tword.replace(\"$$$\",\"#\")] = data[1][0][1][0].replace(\"$$$\",\"#\")\n",
        "      except IndexError:\n",
        "        error_words.add(tword.replace(\"$$$\",\"#\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o7PTqL3YJme"
      },
      "source": [
        "from emoji import UNICODE_EMOJI\n",
        "\n",
        "def is_emoji(s):\n",
        "    for i in s:\n",
        "      \n",
        "      if i in UNICODE_EMOJI['en'] or \"üáÆüá≥\" in s:\n",
        "        return True\n",
        "    return False\n",
        "    \n",
        "removed = []\n",
        "for i in error_words:\n",
        "  if is_emoji(i):\n",
        "    removed.append(i)\n",
        "    transliteration_map[i] = \"‡§á‡§Æ‡•ã‡§ü‡§ø‡§ï‡•â‡§®\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiQkq2MwE6Rq"
      },
      "source": [
        "for i in removed:\n",
        "  error_words.remove(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_5gACM0bnlT"
      },
      "source": [
        "to_remove = [\"¬§¬§¬§¬§¬§¬§\",\"¬•\",\"¬∞\",\"¬≥\",'¬µ',\n",
        "             \"‚Äú\",\"‚Äù\",'‚Äò','‚Äô',\"‚Ä¢\",\"‚Ä¶\",\n",
        "             \"‚Ä¶‚Ä¶‚Ä¶‚Ä¶.....\",'‚Çπ','‚òÖ','‚ô°','‚ô°‚ô°‚ô°',\n",
        "             '‚ô°‚ô°‚ô°‚ô°‚ô°','\\U000fe334\\U000fe334']\n",
        "\n",
        "for i in to_remove:\n",
        "    transliteration_map[i] = \"‡§á‡§Æ‡•ã‡§ü‡§ø‡§ï‡•â‡§®\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkDxW6VEctDu"
      },
      "source": [
        "for i in to_remove:\n",
        "  error_words.remove(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BegoPr6JbgEm"
      },
      "source": [
        "error_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhoo3-WLISdv"
      },
      "source": [
        "transliteration_map[\"&_only\"] =\"&_‡§ì‡§®‡§≤‡•Ä\"\n",
        "transliteration_map[\"&theater\"] = \"&‡§•‡§ø‡§è‡§ü‡§∞\"\n",
        "transliteration_map[\"^exactly\"] = \"^‡§è‡§ï‡•ç‡§∏‡§æ‡§ï‡•ç‡§ü‡•ç‡§≤‡•Ä\"\n",
        "transliteration_map[\"atttttttttttttt\"] = \"‡§Ö‡§ü‡•ç‡§ü\"\n",
        "transliteration_map[\"bhaiiiiiiiiii\"] = \"‡§≠‡§æ‡§à\"\n",
        "transliteration_map[\"bhaiiiiiiiiiii\"] = \"‡§≠‡§æ‡§à\"\n",
        "transliteration_map[\"bhaiiiiiiiiiiii\"] = \"‡§≠‡§æ‡§à\"\n",
        "transliteration_map[\"bhaiiiiiiiiiiiii\"] = \"‡§≠‡§æ‡§à\"\n",
        "transliteration_map[\"bhaiiiiiiiiiiiiiii\"] = \"‡§≠‡§æ‡§à\"\n",
        "transliteration_map[\"bhaiiiiiiiiiiiiiiiiiii\"] = \"‡§≠‡§æ‡§à\"\n",
        "transliteration_map[\"biggggggggggg\"] = \"‡§¨‡§ø‡§ó\"\n",
        "transliteration_map['goooooooooooooo'] = '‡§ú‡•ã'\n",
        "transliteration_map['haaaaaaaaaaa'] = \"‡§π‡•à\"\n",
        "transliteration_map['loveeeeeeeeeeee'] = \"‡§≤‡§µ\"\n",
        "transliteration_map['modijiiiiiiiiiiii'] = \"‡§Æ‡•ã‡§¶‡§ø‡§ú‡§ø\"\n",
        "transliteration_map['oooooooooo'] = \"‡§ì\"\n",
        "transliteration_map['punjabbbbbbbbbbbbb'] = \"‡§™‡§Ç‡§ú‡§æ‡§¨\"\n",
        "transliteration_map['salmaaaaaaaaaaan'] = '‡§∏‡§≤‡§Æ‡§æ‡§®'\n",
        "transliteration_map['sooooooooooo'] = '‡§∏‡•ã'\n",
        "transliteration_map['sooooooooooooo'] = '‡§∏‡•ã'\n",
        "transliteration_map['soooooooooooooooooooooooooooooooooooooooooooooooo'] = '‡§∏‡•ã'\n",
        "transliteration_map['srrrrrrrrrrrr'] ='‡§∏‡§∞'\n",
        "transliteration_map['sryyyyyyyyyyyy'] = '‡§∂‡•ç‡§∞‡•Ä'\n",
        "transliteration_map['thuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu'] = '‡§†‡•Å'\n",
        "transliteration_map['trueeeeeeeeeeee'] = '‡§ü‡•ç‡§∞‡•Ç'\n",
        "transliteration_map['uhhhhhhhhhhhhh'] = '‡§Ø‡§π'\n",
        "transliteration_map['very^infinity'] = '‡§µ‡•à‡§∞‡•Ä^‡§á‡§®‡§´‡§ø‡§®‡•á‡§ü‡§≤‡•Ä'\n",
        "transliteration_map['yrrrrrrrrrr'] = '‡§Ø‡§∞'\n",
        "transliteration_map['khaaaaaaaaaaaaaan'] = '‡§ñ‡§æ‡§®'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCgNo8SmLyyM"
      },
      "source": [
        "for i in transliteration_map:\n",
        "  if i in error_words:\n",
        "    error_words.remove(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu2uqizclx1p"
      },
      "source": [
        "error_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ta9fRtEimEgf"
      },
      "source": [
        "with open(\"trans_map\",\"w\",encoding=\"utf-8\") as f:\n",
        "  json.dump(transliteration_map,f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCgKr1FzL8JR"
      },
      "source": [
        "def apply_trans_map(sentence):\n",
        "  words = []\n",
        "  for word in sentence.split():\n",
        "    if word in transliteration_map:\n",
        "      words.append(transliteration_map[word])\n",
        "    else:\n",
        "      words.append(word)\n",
        "  return \" \".join(words)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-20SjzboROg5"
      },
      "source": [
        "train=pd.read_csv(\"train.csv\")\n",
        "train[\"text\"]=train['text'].map(lambda sent: apply_trans_map(sent))\n",
        "train.to_csv(\"hin_train.csv\",index=False)\n",
        "\n",
        "dev=pd.read_csv(\"dev.csv\")\n",
        "dev[\"text\"]=dev['text'].map(lambda sent: apply_trans_map(sent))\n",
        "dev.to_csv(\"hin_dev.csv\",index=False)\n",
        "\n",
        "test=pd.read_csv(\"test.csv\")\n",
        "test[\"text\"]=test['text'].map(lambda sent: apply_trans_map(sent))\n",
        "test.to_csv(\"hin_test.csv\",index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIk2uP-QSxPa"
      },
      "source": [
        "import json\n",
        "with open(\"hin_map.json\",\"w\") as f:\n",
        "  json.dump(transliteration_map,f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ggl1AG1q9IT"
      },
      "source": [
        "text = test.text.tolist()\n",
        "ans = []\n",
        "with open(\"test.txt\",\"w\") as f:\n",
        "  for i in text:\n",
        "    f.write(\"\\n\".join(i.split())+\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsEWIIlo40sf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07235b81-e227-494b-b2a2-c47bde74f1e4"
      },
      "source": [
        "!wc -l *.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  37906 etest.txt\n",
            "  37906 test.txt\n",
            " 146680 vocab.txt\n",
            " 222492 total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmoSAO1qm5qI",
        "outputId": "f4d8e0e5-2cd3-4e0e-acbb-5e812058d624"
      },
      "source": [
        "test=pd.read_csv(\"hin_test.csv\")\n",
        "text = test.text.tolist()\n",
        "ans = []\n",
        "with open(\"test5.txt\",\"w\") as f:\n",
        "  for i in text:\n",
        "    f.write(\"\\n\".join(i.split())+\"\\n\\n\")\n",
        "!wc -l test5.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37906 test5.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZwrDDTtnZ3a"
      },
      "source": [
        "with open(\"test.txt\") as f:\n",
        "    text = []\n",
        "    sent = []\n",
        "    count = 0\n",
        "    for line in f:\n",
        "        if line != \"\\n\":\n",
        "            sent.append(line.lower().strip(\"\\n\"))\n",
        "            \n",
        "            count += 1\n",
        "        else:\n",
        "            if count == 0:\n",
        "                continue\n",
        "            else:\n",
        "                text.append(\" \".join(sent))\n",
        "                sent = []\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ggdseeup2v-"
      },
      "source": [
        "test = pd.DataFrame({\"text\":text})\n",
        "test.to_csv(\"hin_test.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}